## План обучения

### Этап 1 — основы LLM и API-интеграции
Цель: понимать, что такое LLM, как с ними взаимодействовать через API, базовые параметры (температура, токены, промпты).

Изучить: что такое prompt, system/user messages, токенизация на концептуальном уровне.

Практика: сделать простое приложение с вызовом LLM через HTTP (имитируй внешний API), реализовать обработку входа/выхода и базовый retry/backoff.

Задача: написать FastAPI-эндпойнт /chat который принимает сообщение, формирует prompt и возвращает ответ (можешь мокать модель, если нет API ключа).

Проверка: локальный endpoint возвращает адекватный ответ при разных prompt.


### Этап 2 — работа с текстовыми данными и предобработка
Этап 2 — работа с текстовыми данными и предобработка

Цель: уметь готовить и нормализовать текст для LLM и пайплайнов.

Навыки: очистка текста, токенизация/сегментация, chunking (разбиение больших документов), простая OCR-пайплайн интеграция (если нужно).

Практика: взять набор документов (markdown / pdf / txt), написать скрипт, который разбивает на чанки, хранит метаданные (название, позиция), считает приблизительные токены.

Задача: pipeline ingest_documents(dir) → list(chunks with metadata).

Проверка: можно загружать векторизацию следующий этап.

### Этап 3 — embeddings и векторные базы данных
Цель: уметь строить векторные представления и хранить/поискать по ним.

Изучить: концепт embeddings, cosine similarity, nearest neighbors.

Инструменты: выбрать 1-2 векторных DB (например Qdrant, Chroma, Weaviate) — понять CRUD, поиск, фильтрация по метаданным.

Практика: взять чанки из этапа 2, получить embeddings (через API / локальную библиотеку), загрузить в векторную БД, реализовать поиск по запросу.

Задача: endpoint /search который возвращает релевантные фрагменты по вопросу.

Проверка: при тестовых запросах система возвращает релевантные фрагменты в порядке убывания релевантности.

### Этап 4 — Retrieval-Augmented Generation (RAG)
Цель: объединять векторный поиск + LLM для ответа на запросы с контекстом.

Понимание: как формировать prompt с retrieved контекстом; управление контекстным окном; безопасное включение источников.

Практика: реализовать flow user query → search → construct prompt with top-K chunks → call LLM → return answer + sources.

Задача: endpoint /answer с полным RAG-пайплайном и ссылками на источники (путь к документу/фрагменту).

Проверка: ответы используют фактологический контекст; есть ссылка на источник для каждого существенного утверждения.

### Этап 5 — инструментирование, агентов и оркестрация
Цель: понять, как композиционно вызывать разные инструменты (API, поиск, база данных, внешние сервисы).

Изучить: концепция “agents” — планирование шагов, вызов функций, chain of thought (на прикладном уровне).

Практика: реализовать простого агента, который на основе запроса:

решает, нужен ли поиск/выполнение кода/запрос в БД,

вызывает нужные шаги,

собирает финальный ответ.

Задача: агент, который находит данные в векторной БД и при необходимости вызывает внешний API (пример: получить цену товара + описание и сформировать карточку).

Проверка: агент выполняет несколько шагов и возвращает итоговый агрегированный ответ.

### Этап 6 — безопасность, фильтрация и контроль качества
Цель: научиться ограничивать нежелательные ответы, откатывать hallucinations, логировать и тестировать качество.

Навыки: валидация входа/выхода, blacklist/whitelist, проверка второго прохода factuality (например: подавать на отдельный prompt-валидатор).

Практика: добавить в проект модуль проверки ответов — если уверенность низкая, возвращать «не уверен».

Задача: интегрировать логирование запросов/ответов + простой метрикатор (кол-во запросов, avg длина ответа, частота отказов).

Проверка: система отклоняет подозрительные запросы и логирует события.

### Этап 7 — продакшн: развёртывание, мониторинг и оптимизация затрат
Цель: уметь упаковать, развернуть и поддерживать AI-фичи в продакшне.

Навыки: Docker-compose / K8s минимальные паттерны, CI/CD для сервиса, ограничение rate-limits, кэширование, batching, async обработка.

Практика: создать Docker + простой CI workflow (build, test, push), настроить метрики (prometheus/metrics endpoint) и алерты (например через логирование).

Оптимизация: implement prompt caching, embedding caching, и fallback strategies для экономии токенов.

Задача: продакшн-ready сервис с полосой мониторинга и стратегиями экономии запросов к LLM.

Проверка: сервис запускался в контейнере, есть health endpoint, basic metrics, и пример конфигурации CI.

### Этап 8 — дополнительные навыки (по желанию)
Варианты углубления:

Fine-tuning / adapters / LoRA — если захочешь кастомизировать модель (тут уже чуть больше техники, но можно минимально).

Speech / multimodal интеграции (speech-to-text, image inputs).

Advanced evaluation: A/B тестирование промптов, human-in-the-loop корректировки.

Проверка: реализована 1-2 опциональные фичи в проекте.
